---
title: "Capstone Project  of the Johns Hopkins Coursera 
  Data Science Specialization"
author: "fss14142"
date: "March-April 015"
output: html_document
---

This gh-pages repository contains some additional information about the model I used for the *Capstone Project*  of the 
[Johns Hopkins Coursera Data Science Specialization](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage). The main repostitory with the code of the project is:

[https://github.com/fss14142/CourseraJHUDataScienceCapstone](https://github.com/fss14142/CourseraJHUDataScienceCapstone)


The main goal of the project is to design a [Shiny](http://shiny.rstudio.com/) application that takes as input a partial (incomplete) English sentence and predicts the next word in the sentence. You may want to start by taking a look at the app. In that case, please remember to read the instructions in the *Documentation* tab of the app before using it. The app can be found at:  
[http://fss14142.shinyapps.io/shinyAppTest](http://fss14142.shinyapps.io/shinyAppTest). 

Here you will find theoretical information of the model being constructed, an N-gram model with discounted smoothing and Katz backoff. This theoretical part is specially important because I have found in the Web quite a few wrong descriptions of this type of models. The reason for this is probably that the popular book by Jurafsky and Martin, used to teach many courses on this subject, contains a errata in the formula for this model. See the [http://www.cs.colorado.edu/~martin/SLP/Errata/SLP2-PH-Errata.html](official errata list) for the book here.  Therefore I have decided to include correct versions of the formulas for the model in this document.


## Description of the theoretical model

As I mentioned before, the Katz backoff formulas in many web pages about Natural LAnguage Processing are wrong. These are the corrected formulas I have used for my model:

1. All probabilities for n-grams are computed with a discount smoothing strategy. The particular discounting strategy is not as important as the fact that some probability is left remaining for the unseen n-grams. Let $c(w^j_i)$ and $c^*(w^j_i)$ denote respectively the non-discounted and discounted counts for an N-gram $w^i_j$. Then (see Jurafsky-Martin (2008), Eq. 4.39 in page 140) the non-backed off conditional probabilities for an n-gram are defined as:

\[
P^*(w_n|w^{n-1}_{n-N+1}) = \dfrac{c^*(w^{n}_{n-N+1})}{c(w^{n-1}_{n-N+1})}
\]

2. Besides, the Katz backoff strategy means that a new probability assignment $P_K$ is defined in the case where the n-gram $w^n_1$ has not been seen before. That is, when $c(w^n_1) = 0$. The <font color="red">correct definition</font> is:

\[
P_K(w^n|w^{n-1}_1) = 
\begin{cases}
P^*(w_n|w^{n-1}_{n-N+1}) & \mbox{ if }c(w^n_1) > 0\quad\mbox{(no backoff in this case)}\\[3mm]
\alpha(w^{n-1}_{n-N+1})P_K(w_n|w^{n-1}_{n-N+2})) & \mbox{ else if } c(w^{n-1}_{n-N+1}) >0 \\[3mm]
P_K(w_n|w^{n-1}_{n-N+2})) & \mbox{ otherwise (pure backoff).}
\end{cases}
\]

In the second case we use backoff to an $(n-1)$- gram that we have in fact seen (positive counts) and so we add the (contextual) coefficient $\alpha$ to factor in the knoledge we have about the (n-1)-grams probabilities. That coefficient is defined as follows. First we define:

\[
\beta(w^{n-1}_{n-N+1}) = 1 - \sum_{w_n:c(w^{n}_{n-N+1})>0}P^*(w_n\,|\,w^{n-1}_{n-N+1})
\]
and then
\[
\alpha(w^{n-1}_{n-N+1}) =\dfrac{\beta(w^{n-1}_{n-N+1}) }{ 1 - \sum_{w_n:c(w^{n}_{n-N+1})>0}P^*(w_n\,|\,w^{n-1}_{n-N+2})}
\]
The computation of these $\alpha$ coefficients and the $P^*$ probabilities is the heart of a Katz backoff model.

